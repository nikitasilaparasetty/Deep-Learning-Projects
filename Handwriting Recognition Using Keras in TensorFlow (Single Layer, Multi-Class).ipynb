{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Recognition Using Keras in TensorFlow (Single Layer, Multi-Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For a detailed tutorial of this project, please refer to the book, **\"Machine Learning Concepts with Python and the Jupyter Notebook Environment: Using TensorFlow 2.0\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses the 'MNIST' dataset, which is a set of images of handwritten digits (from 0 to 9). This dataset is pre-divided into 60,000 training images and 10,000 test images, all of size 28x28 pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the TensorFlow Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.mnist\n",
    "(ip_train, op_train),(ip_test, op_test) = data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data to ensure that the pixel values range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_train, ip_test = ip_train/255.0, ip_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the input images (make them one-dimensional)\n",
    "# Add one dense layer\n",
    "# The number of neurons in the dense layer depends on the number of outputs (classes or labels) and hence we put 10\n",
    "# We use the softmax activation function\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28)), \n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7850 (30.66 KB)\n",
      "Trainable params: 7850 (30.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 2s 881us/step - loss: 0.4695 - accuracy: 0.8773\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 2s 810us/step - loss: 0.3042 - accuracy: 0.9143\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 2s 827us/step - loss: 0.2836 - accuracy: 0.9204\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 1s 795us/step - loss: 0.2731 - accuracy: 0.9232\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 2s 823us/step - loss: 0.2667 - accuracy: 0.9256\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 2s 816us/step - loss: 0.2619 - accuracy: 0.9275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x162002b34c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ip_train, op_train, epochs = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 837us/step - loss: 0.2639 - accuracy: 0.9264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2639002799987793, 0.9264000058174133]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ip_test, op_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the image to be tested from the dataset - It can be any value between 0 and 9999 because there are 10,000 test images.\n",
    "\n",
    "test_image = ip_test[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1621ddedcc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image\n",
    "\n",
    "plt.imshow(test_image.reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognize the image to be the number 6. We now need to check if the model is able to recognize this as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy and the image function under keras\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image into a NumPy array\n",
    "\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test image\n",
    "\n",
    "test_image = test_image.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.6346832e-07, 2.2057748e-14, 3.5324938e-05, 3.7419672e-09,\n",
       "        2.0508024e-07, 7.6308006e-06, 9.9995589e-01, 1.0307592e-12,\n",
       "        2.8117238e-08, 7.9961052e-11]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the model to make a prediction, store the value under a variable, and call that variable to display the prediction\n",
    "\n",
    "result = model.predict(test_image)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round off tha array elements\n",
    "\n",
    "np.around(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the element that gives the maximum value among all the elements of the array\n",
    "\n",
    "(np.around(result)).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the highest value (the number 1) is located in the 6th position, which shows that the model has correctly predicted that the test image belongs to the 6th class,\n",
    "that is, the image is that of the number 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
